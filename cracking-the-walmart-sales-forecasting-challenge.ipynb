{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7255a69e-6c53-434f-829d-405ecb3798e0",
    "_uuid": "cd3296c40587f51a36e1818ae4a310ad05b658b5"
   },
   "source": [
    "# The Walmart challenge: Modelling weekly sales\n",
    "In this notebook, we use data from Walmart to forecast their weekly sales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dfc3c722-10c5-46fc-9f8b-c078d99205b8",
    "_uuid": "5c7360ce9d2ba20f564c8c230f4ab44b2b849ea3",
    "collapsed": true
   },
   "source": [
    "## Summary of results and approach\n",
    "\n",
    "Work in Progress:\n",
    "\n",
    "At writing, our internal competition at Bletchley has ended. Interestingly, the winning group had a different approach then would be expected from an AI/Machine Learning bootcamp. Their forecasts were based simply on a median of the weekly sales grouped by the Type of Store, Store & Department number, Month and Holiday dummy. \n",
    "\n",
    "Therefore, in my next approach, the goal will be to improve their results with the help of Neural Networks and other machine learning methods. In fact, the median will be computed similarly to how the winning group did, and a new variable, the difference to the median, will be computed. This difference will be the new dependent variable and will be estimated based on new holiday dummies, markdowns and info on lagged sales data if available.\n",
    "\n",
    "**Final result: MAE dropped from 2200 to 1800.\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "014a771f-ba58-4183-8ca6-301d9c2aa97f",
    "_uuid": "dbb52c9e2a03a83255ba903f498092e9f590ac9a"
   },
   "source": [
    "## Understanding the problem and defining a success metric\n",
    "\n",
    "The problem is quite straightforward. Data from Walmart stores accross the US is given, and it is up to us to forecast their weekly sales. The data is already split into a training and a test set, and we want to fit a model to the training data that is able to forecast those weeks sales as accurately as possible. In fact, our metric of interest will be the [Mean Absolute Error](https://en.wikipedia.org/wiki/Mean_absolute_error). \n",
    "\n",
    "The metric is not very complicated. The further away from the actual outcome our forecast is, the harder it will be punished. Optimally, we exactly predict the weekly sales. This of course is highly unlikely, but we must try to get as close as possible. The base case of our model will be a simple linear regression baseline, which gave a MSE of \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c6b58708-9f9a-42ee-a437-d8ffa169cde0",
    "_uuid": "4a4ac64d7e90f02d683685cf9c9432ebb56e2580"
   },
   "source": [
    "## Load and explore data\n",
    "Before we do anything, lets import some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "7c399d8c-5531-44a8-a758-ef7785518f28",
    "_uuid": "3f34f97043b07eb2c0cca8fdc317a199ace93a6c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Really need these\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from numpy import *\n",
    "\n",
    "\n",
    "#Handy for debugging\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "#Date stuff\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "#Do some statistics\n",
    "from scipy.misc import imread\n",
    "from scipy import sparse\n",
    "import scipy.stats as ss\n",
    "import math\n",
    "\n",
    "#Nice graphing tools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import plotly.offline as py\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "#Machine learning tools\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from scipy import sparse\n",
    "\n",
    "## Keras for deep learning\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "\n",
    "## Performance measures\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bef9a5bc-95a3-4fc6-aed9-2c970946715e",
    "_uuid": "015c160f4bf0f386007e7f1d948c635c169a8a27"
   },
   "source": [
    "## Prepare functions\n",
    "I initialize my functions in the beginning of the script to make the whole seem cleaner.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "2ec4aae6-84a7-42fd-9ad5-18b0c9f664c0",
    "_kg_hide-input": true,
    "_uuid": "a713320913e1420f12d611a3bcd65928621e7a62",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merge info\n",
    "def mergeData(df):\n",
    "    features =pd.read_csv('../input/wallmart-sales-forecast-datasets/features.csv')\n",
    "    storesdata =pd.read_csv('../input/wallmart-sales-forecast-datasets/stores.csv')\n",
    "    df = pd.merge(df, features, on=['Store','Date','IsHoliday'],\n",
    "                  how='inner')\n",
    "    df = pd.merge(df, storesdata, on=['Store'],\n",
    "                  how='inner')\n",
    "    return df\n",
    "\n",
    "#http://scikit-learn.org/stable/auto_examples/plot_cv_predict.html\n",
    "def plot_prediction(predicted,true,desciption):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(true, predicted, edgecolors=(0, 0, 0))\n",
    "    ax.plot([true.min(), true.max()], [true.min(), true.max()], 'k--', lw=4)\n",
    "    ax.set_xlabel('Measured')\n",
    "    ax.set_ylabel('Predicted by '+desciption)\n",
    "    ax.plot([-30,30], [0,0], 'k-')   \n",
    "    ax.plot([0,0], [-30,30], 'k-')\n",
    "    plt.show()\n",
    "def binary(movement):\n",
    "    \"\"\"\n",
    "    Converts percent change to a binary 1 or 0, where 1 is an increase and 0 is a decrease/no change\n",
    "    \n",
    "    \"\"\"\n",
    "    #Empty arrays where a 1 represents an increase in price and a 0 represents a decrease in price\n",
    "    direction = np.empty(movement.shape[0])\n",
    "    #If the change in price is greater than zero, store it as a 1\n",
    "    #If the change in price is less than zero, store it as a 0\n",
    "    for i in range(movement.shape[0]):\n",
    "        if movement[i] > 0:\n",
    "            direction[i] = 1\n",
    "        else:\n",
    "            direction[i]= 0\n",
    "    return direction\n",
    "\n",
    "def scatterplots(feature, label):\n",
    "    x = feature\n",
    "    y = df['Weekly_Sales']\n",
    "    plt.scatter(x, y)\n",
    "    plt.ylabel('sales')\n",
    "    plt.xlabel(label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5b80c98a-8fbc-47b8-92ba-718c0be74dd1",
    "_uuid": "3d232efd3a8d58731842ed29f6e4488917204082"
   },
   "source": [
    "Identify data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "65443d3c-7da7-41b9-83ae-dc9a508d829f",
    "_uuid": "61e0477df81dc91f75f941b39d6673ea8a63e153"
   },
   "outputs": [],
   "source": [
    "print('Reading data...')\n",
    "print(os.listdir('../input/'))\n",
    "print(os.listdir('../input/wallmart-sales-forecast-datasets'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fc4d0a81-91e9-45f1-a575-2054301f1ee0",
    "_uuid": "35263e3a2876a233787e2e7ad04b4905c9659d5b"
   },
   "source": [
    "There are two competitions that have more or less the same data. Choose which competition to participate in.\n",
    "[One](https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting) or [two](https://www.kaggle.com/c/walmart-sales-forecasting). All comments are based on number two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "3b6114c9-5190-4f36-892b-bb812f8eb544",
    "_uuid": "a2bc427a7fc5db309b1fade20e4d704fa6dfffe9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataSource = 2\n",
    "if dataSource==1:\n",
    "    train = mergeData(pd.read_csv('../input/wallmart-sales-forecast-datasets/train.csv'))\n",
    "    test = mergeData(pd.read_csv('../input/wallmart-sales-forecast-datasets/test.csv'))\n",
    "    train['Split'] = 'Train'\n",
    "    test['Split'] = 'Test'\n",
    "    test.head()\n",
    "else: \n",
    "    train = pd.read_csv('../input/course-material-walmart-challenge/train.csv')\n",
    "    test = pd.read_csv('../input/course-material-walmart-challenge/test.csv')\n",
    "    train['Split'] = 'Train'\n",
    "    test['Split'] = 'Test'\n",
    "    test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "04fef74f-4f53-4328-a882-7e3be34c386b",
    "_uuid": "cca3344c7d9c45a2c0dc41fe90e88083e15fb8de"
   },
   "source": [
    "In order to efficiently modify our data, we merge the two datasets for now. We also keep track of the length of our training set so we know how to split it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "c813405d-10a0-45ba-891e-e4e494ab34fc",
    "_uuid": "e6c57c708a9db7fc01447b6b00908b67cb7f4b09"
   },
   "outputs": [],
   "source": [
    "t_len = len(train) # Get number of training examples\n",
    "df = pd.concat([train,test],axis=0) # Join train and test\n",
    "df.tail() # Get an overview of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8bca733e-3ba3-4db2-bcc1-66b371f702ab",
    "_uuid": "adac71f84a15958f3ec618c5c49f8f86acbb9d06"
   },
   "source": [
    "Let's get a clearer image of what our data actually looks like with the describe function. This will give use summary statistics of our numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "7094d024-8782-4d1c-8b46-ddf7140f46ab",
    "_uuid": "b2d03ef65c79c4144d743e6ab70f2de365a26c61",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4e5f019b-4264-4d27-9b69-3b3b57d16e87",
    "_uuid": "db86071226556482ae6dbc13cc3f613c3539a44e"
   },
   "source": [
    "Since we are in the Netherlands, and we don't understand Fahrenheit, let's do a quick change there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "c511049e-674b-4602-9393-5b0a3c38d616",
    "_uuid": "25400d3e46ccfe5172b8588f1cf492190daede13",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Temperature'] = (df['Temperature'] - 32) * 5/9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ff58816e-5ed2-4e08-982a-0b5508f26f4e",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "c39ebb82e23003d33d29b408adbe67bddc65cde0"
   },
   "source": [
    "Although there is not a large variety of variables, we can definitely work with this. In the next section, we will clean the data set, engineer some new features and add dummy variables. For now, let's try to find any obvious relations between our variables to get a feeling for the data. We begin with a correlation matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "4abbc5ad-f380-429e-b8d8-10917e8961ec",
    "_uuid": "4af4dc2d78fa747d7a835773e35048f7dbb401fb"
   },
   "outputs": [],
   "source": [
    "# Code from https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eded2bed-76d0-431c-9163-1ade898383fc",
    "_uuid": "1c22efea7c198086cf23322fa0209b9569b15e68"
   },
   "source": [
    "Most of what we see in the correlation table is of little surprise. Discounts are correlated and higher unemployment means lower Consumer Price Index. More interestingly, it appears that higher department numbers have higher sales. Maybe because they are newer? Also, larger stores generate more sales, discounts generally generate higher sales values and larger unemployment result in a bit fewer sales. Unfortunately, there appears to be little relationship between holidays, temperatures or fuelprices with our weekly sales.\n",
    "\n",
    "Next up, let's plot some of these relationships to get a clearer image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "d28d3d3d-dea6-4dd6-8f85-bbc47c6d9c89",
    "_uuid": "4e438a2c22546a83c92950ddbd9921ce0763b99e"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "headers = list(df)\n",
    "labels = headers\n",
    "scatterplots(df['Fuel_Price'], 'Fuel_Price')\n",
    "scatterplots(df['Size'], 'Size')\n",
    "#scatterplots(df['Temperature'], 'Temperature')\n",
    "#scatterplots(df['Unemployment'], 'Unemployment')\n",
    "#scatterplots(df['IsHoliday'], 'IsHoliday')\n",
    "#scatterplots(df['Type'], 'Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "14c68f69-2452-4078-95e1-013515ecc760",
    "_uuid": "4a892b0728f61c43132b845fd2ca772fa564598c"
   },
   "source": [
    "From this plot, we notice that type C stores have fewer sales in general and holidays clearly show more sales.Although no further relationships appear evident from this analysis, there appears to be some outliers in our data. Let's take a bit of a closer look at these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "dda5531a-c622-409e-8c7c-852fbea528d2",
    "_uuid": "a302b87ee2f762c618cf068e3bd085f58143e1e3"
   },
   "outputs": [],
   "source": [
    "df.loc[df['Weekly_Sales'] >300000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5eae0c43-ccfa-4e1b-97e8-d457f2bf5eee",
    "_uuid": "d14d5237ba53fc3181f51b1c15d4bdcf9239549c"
   },
   "source": [
    "It appears to be quite obvious. The end of November sees a lot of exceptionally large sales. This special day, better known as Black friday, causes sales to be on fire, and undoubtedly a dummy variable should be created for this day. Also, Christmas, appears here and there. Since it is not considered holiday, we will also make a dummy for this day. Let's see if we should consider some other special days as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "dcf43c7b-2bc3-4bbc-a28c-2145c909ef55",
    "_uuid": "c2d02c25b2f203cf39b29de7c58f70f8881042d2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[df['Weekly_Sales'] >240000,\"Date\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "688a6ff2-8b99-47d6-b622-c98a726de744",
    "_uuid": "5111380a08a7718088eecf12e3c1a9017e67c5ea"
   },
   "source": [
    "Except for a handful spurious other dates, it appears that the two days before Christmas and Black Friday will do the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39f735a8-adfd-4e56-9974-4425a5e482d7",
    "_uuid": "f774dcb9a50a4dea6ae2042411f4a5c6ce59a667"
   },
   "source": [
    "\n",
    "\n",
    "## Scrub the data and engineer features\n",
    "\n",
    "### Missing values\n",
    "\n",
    "We will start with filling in any blank values. There seem to be some missing values in the data. We have to make sure to deal with them before feeding anything into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "4c2f78ee-4bf8-4d90-9e61-611aed761c1b",
    "_uuid": "69a906f1e30491b5e20eadc6a807862ee4c4a344",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "703abe3a-cbd5-4b81-8751-92c11d85d489",
    "_uuid": "51e0995047f008d796329e30e1b8d38e8040c124"
   },
   "source": [
    "We will do a bit of very basic feature engineering here by creating a feature which indicates whether a certain markdown was active at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "90dbdf5c-2de5-493b-a846-3db0e9a2641e",
    "_uuid": "b9a1c793ded40b597f959abcaaf8cebec9ee0423",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.assign(md1_present = df.MarkDown1.notnull())\n",
    "df = df.assign(md2_present = df.MarkDown2.notnull())\n",
    "df = df.assign(md3_present = df.MarkDown3.notnull())\n",
    "df = df.assign(md4_present = df.MarkDown4.notnull())\n",
    "df = df.assign(md5_present = df.MarkDown5.notnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "387023f1-dc2b-4b54-8483-6401d24b0d64",
    "_uuid": "9f8d42ccef4c37705e8dc64fafb64e3a45104473"
   },
   "source": [
    "We can probably safely fill all missing values with zero. For the markdowns this means that there was no markdown. For the weekly sales, the missing values are the ones we have to predict, so it does not really matter what we fill in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "690b5760-b2c9-4471-8355-fc9b3b0e0d73",
    "_uuid": "5868b63dfc643d6d8015267c10c8105357de18fc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a237b1b4-8001-4ad3-a8b1-784f407fb768",
    "_uuid": "2401b7681253b94de41c9cbbda369591cc68918e"
   },
   "source": [
    "### Dummy variables: Categorical Data\n",
    "\n",
    "Now we have to create some dummy variebles for categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "75c044d2-4552-4371-91ce-563e8f2f305d",
    "_uuid": "98780a9a5fe53e4944b96cc6417b3c23331fa6ce",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure we can later recognize what a dummy once belonged to\n",
    "df['Type'] = 'Type_' + df['Type'].map(str)\n",
    "df['Store'] = 'Store_' + df['Store'].map(str)\n",
    "df['Dept'] = 'Dept_' + df['Dept'].map(str)\n",
    "df['IsHoliday'] = 'IsHoliday_' + df['IsHoliday'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "9501b560-9dd0-4cbc-9ccd-b517c54c1e58",
    "_uuid": "e4eb22919e3d570949db155871eb1f459b218348",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummies\n",
    "type_dummies = pd.get_dummies(df['Type'])\n",
    "store_dummies = pd.get_dummies(df['Store'])\n",
    "dept_dummies = pd.get_dummies(df['Dept'])\n",
    "holiday_dummies = pd.get_dummies(df['IsHoliday'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3a8c1806-f457-4711-8e8e-11dcbbdb32ab",
    "_uuid": "ced7e187ea4435f8ac2a09368a4545342b787760"
   },
   "source": [
    "### Dummy variables: Dates\n",
    "\n",
    "From our earlier analysis, it has turned out that the date may be our best friend. As a general rule, it is a good start to already distinguish between different months in our model. This will create 12 dummy variables; one for each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "67cbfda4-1074-498f-9f0e-abb41523807f",
    "_uuid": "f95a5a2926c412f0da7cbf1ffd5257031fa67130",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['DateType'] = [datetime.strptime(date, '%Y-%m-%d').date() for date in df['Date'].astype(str).values.tolist()]\n",
    "df['Month'] = [date.month for date in df['DateType']]\n",
    "df['Month'] = 'Month_' + df['Month'].map(str)\n",
    "Month_dummies = pd.get_dummies(df['Month'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "37956ac5-0053-43c5-ad33-50964fdb6ea4",
    "_uuid": "af711b6e8fc0d29439122879d2d0ca3cdb16b6a5"
   },
   "source": [
    "Next, let's look at 'special dates'. One variable for Christmas, one for black friday. We have to manually look up the dates of black friday if we want to extrapolate our data to other years, but for now we know: 26 - 11 - 2010 and 25 - 11 - 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "7e7dae6b-5180-463b-9d96-27e69a8f6f89",
    "_uuid": "ab8bb7940ae5a6ebcca99cb8e375ceb2d80f8b19",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Black_Friday'] = np.where((df['DateType']==datetime(2010, 11, 26).date()) | (df['DateType']==datetime(2011, 11, 25).date()), 'yes', 'no')\n",
    "df['Pre_christmas'] = np.where((df['DateType']==datetime(2010, 12, 23).date()) | (df['DateType']==datetime(2010, 12, 24).date()) | (df['DateType']==datetime(2011, 12, 23).date()) | (df['DateType']==datetime(2011, 12, 24).date()), 'yes', 'no')\n",
    "df['Black_Friday'] = 'Black_Friday_' + df['Black_Friday'].map(str)\n",
    "df['Pre_christmas'] = 'Pre_christmas_' + df['Pre_christmas'].map(str)\n",
    "Black_Friday_dummies = pd.get_dummies(df['Black_Friday'] )\n",
    "Pre_christmas_dummies = pd.get_dummies(df['Pre_christmas'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "a074776c-7611-47e9-93dc-3670a93bfb4e",
    "_uuid": "d2a1eb2f06304e674d02fa4a849a4ccf6dfcf08d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add dummies\n",
    "# We will actually skip some of these\n",
    "#df = pd.concat([df,type_dummies,store_dummies,dept_dummies,holiday_dummies,Pre_christmas_dummies,Black_Friday_dummies,Month_dummies],axis=1)\n",
    "\n",
    "df = pd.concat([df,holiday_dummies,Pre_christmas_dummies,Black_Friday_dummies],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "382a1a31-2659-437a-84d7-999385f894c6",
    "_uuid": "c0cc9cde9f9dfae6da25336c04c79e1d45d6459e"
   },
   "source": [
    "> ### Store median\n",
    "\n",
    "We will take the store median in the available data as one of its properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "86366382-9a01-430f-ab80-8c509b388265",
    "_uuid": "4ae0d5448ea79dd51133285114ef66763e8c3aeb"
   },
   "outputs": [],
   "source": [
    "# Get dataframe with averages per store and department\n",
    "medians = pd.DataFrame({'Median Sales' :df.loc[df['Split']=='Train'].groupby(by=['Type','Dept','Store','Month','IsHoliday'])['Weekly_Sales'].median()}).reset_index()\n",
    "medians.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_cell_guid": "52c88d6f-c069-415d-9acd-619cacdfe50a",
    "_uuid": "f1f136314bc5a9bf038a7db33266a09822a44673",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge by type, store, department and month\n",
    "df = df.merge(medians, how = 'outer', on = ['Type','Dept','Store','Month','IsHoliday'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "1fae052c-9995-4ac4-8dc8-2fff168cee6e",
    "_uuid": "c23b9f955d3047468135eb95040bd13e1d7e4a35",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill NA\n",
    "df['Median Sales'].fillna(df['Median Sales'].loc[df['Split']=='Train'].median(), inplace=True) \n",
    "\n",
    "# Create a key for easy access\n",
    "\n",
    "df['Key'] = df['Type'].map(str)+df['Dept'].map(str)+df['Store'].map(str)+df['Date'].map(str)+df['IsHoliday'].map(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_cell_guid": "7caa328a-aa5c-43e0-9d9c-8ddfbfe2cfdd",
    "_uuid": "f538b52a36a0a045a49c68478f0f39dd3c27cdda"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "882024a8-8579-4c12-926d-e24790e8e7a9",
    "_uuid": "d475888086c9354900408b59e79458957fa9f8ef"
   },
   "source": [
    "### Lagged Variables\n",
    "\n",
    "We will take a lagged variable of our store's previous weeks sales. To do so, we will first add a column with a one week lagged date, sort the data, and then match the lagged sales with the initial dataframe using the department and store number.\n",
    "\n",
    "We begin by adding a column with a one week lag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_cell_guid": "04979adf-e73f-4c3c-ad82-99394cf9c848",
    "_uuid": "1db8e347f7dcc3cfe4d4b8499d55c8ba5eca622d"
   },
   "outputs": [],
   "source": [
    "# Attach variable of last weeks time\n",
    "df['DateLagged'] = df['DateType']- timedelta(days=7)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d4dc55db-5698-4c53-9129-e4d176651d36",
    "_uuid": "17d292e1e0c9a34ab39b7248bf33bb24e0731c51"
   },
   "source": [
    "Next, we create a sorted dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_cell_guid": "656777c7-28f9-4bbc-832e-437421ce60ff",
    "_uuid": "fd8fa3b6b46192d1da45e28cfd964f3e98fa2b6b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a sorted dataframe. This will allow us to find lagged variables much faster!\n",
    "sorted_df = df.sort_values(['Store', 'Dept','DateType'], ascending=[1, 1,1])\n",
    "sorted_df = sorted_df.reset_index(drop=True) # Reinitialize the row indices for the loop to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1e289d20-9327-45d5-890d-db045ee4af1c",
    "_uuid": "057d70644bdbc25238f26bb48d57c467940abc5e"
   },
   "source": [
    "Loop over its rows and check at each step if the previous week's sales are available. If not, fill with store and department average, which we retrieved before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "31370ac7-727d-4a46-9543-d9cee49716ef",
    "_uuid": "ea79299edffdfaa429625e01835c95e6f367f19f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_df['LaggedSales'] = np.nan # Initialize column\n",
    "sorted_df['LaggedAvailable'] = np.nan # Initialize column\n",
    "last=df.loc[0] # intialize last row for first iteration. Doesn't really matter what it is\n",
    "row_len = sorted_df.shape[0]\n",
    "for index, row in sorted_df.iterrows():\n",
    "    lag_date = row[\"DateLagged\"]\n",
    "    # Check if it matches by comparing last weeks value to the compared date \n",
    "    # And if weekly sales aren't 0\n",
    "    if((last['DateType']== lag_date) & (last['Weekly_Sales']>0)): \n",
    "        sorted_df.set_value(index, 'LaggedSales',last['Weekly_Sales'])\n",
    "        sorted_df.set_value(index, 'LaggedAvailable',1)\n",
    "    else:\n",
    "        sorted_df.set_value(index, 'LaggedSales',row['Median Sales']) # Fill with median\n",
    "        sorted_df.set_value(index, 'LaggedAvailable',0)\n",
    "\n",
    "    last = row #Remember last row for speed\n",
    "    if(index%int(row_len/10)==0): #See progress by printing every 10% interval\n",
    "        print(str(int(index*100/row_len))+'% loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_cell_guid": "40a5ab19-9277-44b5-ac17-7d99c888b9c0",
    "_uuid": "0c383fdd3e1e2989217fb3e2a8d0aeb9acbdb6b1"
   },
   "outputs": [],
   "source": [
    "sorted_df[['Dept', 'Store','DateType','LaggedSales','Weekly_Sales','Median Sales']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "85debbad-7690-45a9-a307-091092c21544",
    "_uuid": "b9742e76e055bd2f25d8e97cf75fdffeda13b7dd"
   },
   "source": [
    "Now, merge this new info with our existing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_cell_guid": "b35ee87d-f3e7-44e8-bc77-c2b959b15d18",
    "_uuid": "a1c186a16d976b3eaa7e87697f708c41ee31814b",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge by store and department\n",
    "df = df.merge(sorted_df[['Dept', 'Store','DateType','LaggedSales','LaggedAvailable']], how = 'inner', on = ['Dept', 'Store','DateType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "69d6d569-ed72-4ce1-ade8-fee8e8aa68fc",
    "_uuid": "25128c828cf20241646c95b4ba029b5674b0056e"
   },
   "outputs": [],
   "source": [
    "df['Sales_dif'] = df['Median Sales'] - df['LaggedSales']\n",
    "df[['Dept', 'Store','DateType','LaggedSales','Weekly_Sales','Median Sales']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "026b4c48-8f6b-4ea9-82b3-17444f916d52",
    "_uuid": "ed8c998963b12a949be33d494d5b541509ea7170"
   },
   "source": [
    "### Remove redundant items\n",
    "\n",
    "We will take the store average in the available data as one of its properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_cell_guid": "7562e112-a28e-4c8d-92c2-8f8c0c588c32",
    "_uuid": "879236426c9c5db789a1a051de66a5baed4dd34d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "switch= 1\n",
    "\n",
    "if(switch):\n",
    "    df_backup = df\n",
    "else:\n",
    "    df=df_backup\n",
    "    display(df_backup.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9e30259c-a9c9-4512-8110-c3552e70f6a9",
    "_uuid": "aa28b9433c8f292436ddaa4dbae89798b67c353e"
   },
   "source": [
    "### Scale Variables\n",
    "\n",
    "To make the job of our models easier in the next phase, we normalize our continous data. This is also called feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_cell_guid": "d614753c-3086-4a2e-9c68-8f6dc795859c",
    "_uuid": "bf227383541055c70aee4769c6c5ffd62a7eff1e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df['Unemployment'] = (df['Unemployment'] - df['Unemployment'].mean())/(df['Unemployment'].std())\n",
    "##df['Temperature'] = (df['Temperature'] - df['Temperature'].mean())/(df['Temperature'].std())\n",
    "#df['Fuel_Price'] = (df['Fuel_Price'] - df['Fuel_Price'].mean())/(df['Fuel_Price'].std())\n",
    "#df['CPI'] = (df['CPI'] - df['CPI'].mean())/(df['CPI'].std())\n",
    "#df['MarkDown1'] = (df['MarkDown1'] - df['MarkDown1'].mean())/(df['MarkDown1'].std())\n",
    "#df['MarkDown2'] = (df['MarkDown2'] - df['MarkDown2'].mean())/(df['MarkDown2'].std())\n",
    "#df['MarkDown3'] = (df['MarkDown3'] - df['MarkDown3'].mean())/(df['MarkDown3'].std())\n",
    "#df['MarkDown4'] = (df['MarkDown4'] - df['MarkDown4'].mean())/(df['MarkDown4'].std())\n",
    "#df['MarkDown5'] = (df['MarkDown5'] - df['MarkDown5'].mean())/(df['MarkDown5'].std())\n",
    "#df['LaggedSales']= (df['LaggedSales'] - df['LaggedSales'].mean())/(df['LaggedSales'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a6f2538f-4ea4-47e3-aed4-63196f658d0c",
    "_uuid": "a85252d8752600c322b68945092d7aac63f932ff"
   },
   "source": [
    "Now, let's change the variable to be forecasted to the difference from the median. Afterward, we can drop the weekly sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_cell_guid": "e8179fdb-8c6e-4f54-9444-2d8a3447cefe",
    "_uuid": "38619f3351d17fbeb5da64766aca53e78b24f4aa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Difference'] = df['Median Sales'] - df['Weekly_Sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d18c24ee-d81a-40c5-91dc-8095614ba040",
    "_uuid": "91dfd50c5793354a48700e273c5659f9026e337c"
   },
   "source": [
    "Let's have a look at our data set before running our actual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "9b8f5c99-fd52-4ab8-838d-b969648703d0",
    "_uuid": "8104a375ba22abe1059ce4c90d5526ce69492e7a"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "702b605e-b4e4-4714-9816-f116bc26c0e0",
    "_uuid": "83fb83fe08e7612d1a209fea36a1347ec46ebd84"
   },
   "outputs": [],
   "source": [
    "# Code from https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "359a7deb-1b6c-4e46-8d22-e350a4a98f2e",
    "_uuid": "6b694a6d6061160086840fc79457581a1ae385cb"
   },
   "source": [
    "### Select variables to include in model\n",
    "\n",
    "In this section, we can change the variables we ultimately want to include in our model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_cell_guid": "bc8a320e-7861-499e-b599-f5903a1f3650",
    "_uuid": "45707cd233f4c182591d1a3adc13666db8ba3bb0"
   },
   "outputs": [],
   "source": [
    "selector = [\n",
    "    #'Month',\n",
    "    'CPI',\n",
    "    'Fuel_Price',\n",
    "    'MarkDown1',\n",
    "    'MarkDown2',\n",
    "    'MarkDown3',\n",
    "    'MarkDown4',\n",
    "    'MarkDown5',\n",
    "    'Size',\n",
    "    'Temperature',\n",
    "    'Unemployment',\n",
    "    \n",
    "    \n",
    "    \n",
    "    'md1_present',\n",
    "    'md2_present',\n",
    "    'md3_present',\n",
    "    'md4_present',\n",
    "    'md5_present',\n",
    "\n",
    "    'IsHoliday_False',\n",
    "    'IsHoliday_True',\n",
    "    'Pre_christmas_no',\n",
    "    'Pre_christmas_yes',\n",
    "    'Black_Friday_no',\n",
    "    'Black_Friday_yes',    \n",
    "    'LaggedSales',\n",
    "    'Sales_dif',\n",
    "    'LaggedAvailable'\n",
    "    ]\n",
    "display(df[selector].describe())\n",
    "display(df[selector].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "146c8730-4139-4a2c-8b7b-2878fcac8c9b",
    "_uuid": "2880be4b30dad2cef5c6d92d0cbefe1cf8dde843"
   },
   "source": [
    "### Split data into training and test sets\n",
    "\n",
    "Now we can split train test again and of course remove the trivial weekly sales data from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_cell_guid": "99c55060-308e-4946-bb70-8d542a7d349f",
    "_uuid": "a929329c561f59e9ee2651aeb7f0247cbff2496b"
   },
   "outputs": [],
   "source": [
    "train = df.loc[df['Split']=='Train']\n",
    "test = df.loc[df['Split']=='Test']\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "464d2a3e-4070-4a20-82f5-23fdfb06ddf9",
    "_uuid": "17e12ead60c725b0ba54f76e9614c06f457ec0ae"
   },
   "source": [
    "### Test - dev\n",
    "\n",
    "Usually, model performance can be evaluated on the out-of-sample test set. However, since that data is not available, it may be wise to split our training set one more time in order to be able to test out of sample performance. Let's give up 20% of our training set for this sanity check development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_cell_guid": "12ffbd03-f24f-4864-a694-b2ab788a86bb",
    "_uuid": "dd5aa218523746c09898312ae595094448a0d974"
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducability \n",
    "np.random.seed(42)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(train[selector], train['Difference'], test_size=0.2, random_state=42)\n",
    "print(X_dev.shape)\n",
    "print(y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "70f65c84-790c-41e9-93a9-4ead6c2ee676",
    "_uuid": "dd252e577ea69f0d9e2d187c00db6150388860e0"
   },
   "source": [
    "## Model selection\n",
    "\n",
    "As usual, let's start off with all our imports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7455bcc6-dc16-416e-92b0-30050e72da48",
    "_uuid": "f504f1608bcc9966b2e337d6533c7e14792ae93e"
   },
   "source": [
    "### Adam optimizer with regularization\n",
    "\n",
    "In our next model, we will stick with the relu activator, but replace the momentum with an Adam optimizer. Adaptive momumtum estimator uses exponentially weighted averages of the gradients to optimize its momentum.  However, since this method is known to overfit the model because of its fast decent, we will make use of a regulizer to avoid overfitting. The l2 regulizer adds the sum of absolute values of the weights to the loss function, thus discouraging large weights that overemphasize single observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "39b86e13-054c-43c5-afad-d3939233997b",
    "_uuid": "2935031afc6efb8dad4a94b116d7be0a5a9be76c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neural = False\n",
    "if neural:\n",
    "    # Sequential model\n",
    "    \n",
    "    adam_regularized = Sequential()\n",
    "\n",
    "    # First hidden layer now regularized\n",
    "    adam_regularized.add(Dense(32,activation='relu',\n",
    "                    input_dim=X_train.shape[1],\n",
    "                    kernel_regularizer = regularizers.l2(0.01)))\n",
    "\n",
    "    # Second hidden layer now regularized\n",
    "    adam_regularized.add(Dense(16,activation='relu',\n",
    "                       kernel_regularizer = regularizers.l2(0.01)))\n",
    "\n",
    "    # Output layer stayed sigmoid\n",
    "    adam_regularized.add(Dense(1,activation='linear'))\n",
    "\n",
    "    # Setup adam optimizer\n",
    "    adam_optimizer=keras.optimizers.Adam(lr=0.01,\n",
    "                    beta_1=0.9, \n",
    "                    beta_2=0.999, \n",
    "                    epsilon=1e-08)\n",
    "\n",
    "    # Compile the model\n",
    "    adam_regularized.compile(optimizer=adam_optimizer,\n",
    "                  loss='mean_absolute_error',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    # Train\n",
    "    history_adam_regularized=adam_regularized.fit(X_train, y_train, # Train on training set\n",
    "                                 epochs=10, # We will train over 1,000 epochs\n",
    "                                 batch_size=2048, # Batch size \n",
    "                                 verbose=0) # Suppress Keras output\n",
    "    adam_regularized.evaluate(x=X_dev,y=y_dev)\n",
    "\n",
    "    # Plot network\n",
    "    plt.plot(history_adam_regularized.history['loss'], label='Adam Regularized')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    y_pred_neural = adam_regularized.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3ce5c4ea-86b5-43d0-9624-d9876dde8f07",
    "_uuid": "6fb190992ee0a69b21c9f65318ffac28538e4b98"
   },
   "source": [
    "### Random Forest\n",
    "\n",
    "Train on random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e78ee4f6-3fdb-4f35-bd6c-d65c98ab9054",
    "_kg_hide-output": true,
    "_uuid": "6835aed446aefbf77b31fee142339bb3f0eafa3b"
   },
   "outputs": [],
   "source": [
    "#Random forest model specification\n",
    "regr = RandomForestRegressor(n_estimators=20, criterion='mse', max_depth=None, \n",
    "                      min_samples_split=2, min_samples_leaf=1, \n",
    "                      min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "                      max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                      min_impurity_split=None, bootstrap=True, \n",
    "                      oob_score=False, n_jobs=1, random_state=None, \n",
    "                      verbose=2, warm_start=False)\n",
    "\n",
    "#Train on data\n",
    "regr.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "440a77e1-89cc-4bca-9bca-53119d8cbabc",
    "_uuid": "64b9502fd367d635ffe50502a5e1883b626d99d1"
   },
   "source": [
    "### Model evaluation\n",
    " \n",
    " To evaluate the model, we will look at MAE and accuracy in terms of the number of times it correctly estimated an upward or downward deviation from the median.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1d5e0fbe-ce95-4e93-90de-d5cd7e8e9da6",
    "_uuid": "2ac4ef693daae3535c23b5fe68cffa9ade422562",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_random = regr.predict(X_dev)\n",
    "\n",
    "y_dev = y_dev.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "32a330c0-9eb9-4060-a43b-9969b70acb8f",
    "_uuid": "9321e17ef71c5b965ee9f3b8586c96535febc114",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform forest predictions to observe direction of change\n",
    "direction_true1= binary(y_dev.values)\n",
    "direction_predict = binary(y_pred_random)\n",
    "\n",
    "## show confusion matrix random forest\n",
    "cnf_matrix = confusion_matrix(direction_true1, direction_predict)\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax = sns.heatmap(cnf_matrix, ax=ax, cmap=plt.cm.Greens, annot=True)\n",
    "#ax.set_xticklabels(abbreviation)\n",
    "#ax.set_yticklabels(abbreviation)\n",
    "plt.title('Confusion matrix of random forest predictions')\n",
    "plt.ylabel('True category')\n",
    "plt.xlabel('Predicted category')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_cell_guid": "32929edd-9424-411b-8ef1-75787f4f5d20",
    "_uuid": "4269f1dcf39fcb5dc3b04b2982005cb65eaef726"
   },
   "outputs": [],
   "source": [
    "y_dev['Predicted'] = y_pred_random\n",
    "df_out = pd.merge(train,y_dev[['Predicted']],how = 'left',left_index = True, right_index = True,suffixes=['_True','_Pred'])\n",
    "df_out = df_out[~pd.isnull(df_out['Predicted'])]\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "199b74b6-839a-4981-9139-1fb9c3f1ed8d",
    "_uuid": "a6685526123c88c426ca52abb06350f3bf2b5fb6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_out['prediction'] = df_out['Median Sales']-df_out['Predicted']\n",
    "plot_prediction(df_out['Weekly_Sales'],df_out['prediction'],\"Random Forest\")\n",
    "plot_prediction(y_pred_random,y_dev['Difference'].values,\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a8d32bc0-1fad-4990-893b-3d95730878c6",
    "_uuid": "41833045957ccb7bb9037f4460e9fd54fdc66fa1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Medians: \"+str(sum(abs(df_out['Difference']))/df_out.shape[0]))\n",
    "print(\"Random Forest: \"+str(sum(abs(df_out['Weekly_Sales']-df_out['prediction']))/df_out.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1c8bb655-4836-4135-bd4d-7f9857a8cc0b",
    "_uuid": "2ad109a7af90ed39546cc9765251f49e6783da72"
   },
   "source": [
    "Looks good! Let's train on our full data set to get the maximum amount of information in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3083f91c-6cde-40bc-ad9a-bddeb9fc1dde",
    "_uuid": "43a37e996eedf52be583a5a71b54175bd3ec02d2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Random forest model specification. Set n_estimators lower for faster performance\n",
    "rf_model = RandomForestRegressor(n_estimators=80, criterion='mse', max_depth=None, \n",
    "                      min_samples_split=2, min_samples_leaf=1, \n",
    "                      min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "                      max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                      min_impurity_split=None, bootstrap=True, \n",
    "                      oob_score=False, n_jobs=1, random_state=None, \n",
    "                      verbose=0, warm_start=False)\n",
    "\n",
    "#Train on data\n",
    "rf_model.fit(train[selector], train['Difference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1ab13f08-3cbe-49dc-b6c6-a4d12726964a",
    "_uuid": "111a9fa4c95d12e5867d0d3b8d46648a41362eae",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use if large model skipped\n",
    "#rf_model = regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "70761e61-5956-4731-87a9-29826e2914a6",
    "_uuid": "e3c30e44178c49e849d2bc10931a3cce1492b55b"
   },
   "source": [
    "## Forecasting sales\n",
    "\n",
    "After we have created our model, we can predict things with it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "410fbefc-a1e3-4370-9efc-959168d9d970",
    "_uuid": "5bd5c5ab3a2a37175273595ce322f74695a984cd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_y_prediction = rf_model.predict(test[selector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e3b3c729-df2c-4dcd-a34f-e5350db46ad2",
    "_uuid": "0c431791e2d4d826065abe52b6227c105070b936",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testfile = pd.concat([test.reset_index(drop=True), pd.DataFrame(final_y_prediction)], axis=1)\n",
    "testfile['prediction'] = testfile['Median Sales']-testfile[0]\n",
    "testfile.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "488e96f0-48ad-4cf6-a824-55cd39d2e45f",
    "_uuid": "7c8c55cc85f67e658a6ec5a9f1725bef665212ee"
   },
   "source": [
    "Now we create the submission. Once you run the kernel you can download the submission from its outputs and upload it to the Kaggle InClass competition page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "982cced0-7d5f-49db-baea-bab0f7c119b8",
    "_uuid": "f74aaf3d356cbf771e6a1c701831ac96a3b8a4d5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id':pd.Series([''.join(list(filter(str.isdigit, x))) for x in testfile['Store']]).map(str) + '_' +\n",
    "                           pd.Series([''.join(list(filter(str.isdigit, x))) for x in testfile['Dept']]).map(str)  + '_' +\n",
    "                           testfile['Date'].map(str),\n",
    "                          'Weekly_Sales':testfile['prediction']})\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8a336795-931d-4684-80af-c234f413e611",
    "_uuid": "a9c5897cbf36c2686ff8bf35b3042dc28fa31d3c"
   },
   "source": [
    "Check submission one more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2138ca94-82cc-4a2f-b754-4a3fb6662541",
    "_uuid": "a5549a653482844d45f3812edf87050c08b82cde",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
